/*
 * Copyright Â© 2020 Peter M. Stahl pemistahl@gmail.com
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either expressed or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

use crate::alphabet::Alphabet;
use crate::constant::{
    CHARS_TO_LANGUAGES_MAPPING, JAPANESE_CHARACTER_SET, LANGUAGE_MODELS_DIRECTORY,
    MULTIPLE_WHITESPACE, NO_LETTER, NUMBERS, PUNCTUATION,
};
use crate::language::Language;
use crate::language::Language::*;
use crate::model::{TestDataLanguageModel, TrainingDataLanguageModel};
use crate::ngram::Ngram;
use include_dir::Dir;
use itertools::Itertools;
use once_cell::sync::OnceCell;
use std::cmp::Ordering;
use std::collections::{BTreeMap, HashMap, HashSet};
use std::hash::Hash;
use std::io::{Cursor, Read};
use strum::IntoEnumIterator;
use zip::ZipArchive;

pub struct LanguageDetector {
    languages: HashSet<Language>,
    minimum_relative_distance: f64,
    languages_with_unique_characters: HashSet<Language>,
    one_language_alphabets: HashMap<Alphabet, Language>,
    unigram_language_models: HashMap<Language, TrainingDataLanguageModel>,
    bigram_language_models: HashMap<Language, TrainingDataLanguageModel>,
    trigram_language_models: HashMap<Language, TrainingDataLanguageModel>,
    quadrigram_language_models: HashMap<Language, TrainingDataLanguageModel>,
    fivegram_language_models: HashMap<Language, TrainingDataLanguageModel>,
}

impl LanguageDetector {
    pub(crate) fn from(languages: HashSet<Language>, minimum_relative_distance: f64) -> Self {
        let languages_with_unique_characters = languages
            .iter()
            .filter(|it| it.unique_characters().is_some())
            .cloned()
            .collect();
        let one_language_alphabets = Alphabet::all_supporting_single_language()
            .into_iter()
            .filter(|(_, language)| languages.contains(language))
            .collect();
        Self {
            languages,
            minimum_relative_distance,
            languages_with_unique_characters,
            one_language_alphabets,
            unigram_language_models: hashmap!(),
            bigram_language_models: hashmap!(),
            trigram_language_models: hashmap!(),
            quadrigram_language_models: hashmap!(),
            fivegram_language_models: hashmap!(),
        }
    }

    pub fn detect_language_of<T: Into<String>>(&mut self, text: T) -> Option<Language> {
        let confidence_values = self.compute_language_confidence_values(text);

        if confidence_values.is_empty() {
            return None;
        }

        let (most_likely_language, most_likely_language_probability) =
            &confidence_values.first().unwrap();

        if confidence_values.len() == 1 {
            return Some(most_likely_language.clone());
        }

        let (_, second_most_likely_language_probability) = &confidence_values.get(1).unwrap();

        if most_likely_language_probability == second_most_likely_language_probability {
            return None;
        }

        if (most_likely_language_probability - second_most_likely_language_probability)
            < self.minimum_relative_distance
        {
            return None;
        }

        Some(most_likely_language.clone())
    }

    pub fn compute_language_confidence_values<T: Into<String>>(
        &mut self,
        text: T,
    ) -> Vec<(Language, f64)> {
        let mut values = vec![];
        let cleaned_up_text = self.clean_up_input_text(text.into());

        if cleaned_up_text.is_empty() || NO_LETTER.is_match(&cleaned_up_text) {
            return values;
        }

        let words = self.split_text_into_words(&cleaned_up_text);
        let language_detected_by_rules = self.detect_language_with_rules(&words);

        if let Some(language) = language_detected_by_rules {
            values.push((language, 1.0));
            return values;
        }

        let mut filtered_languages = self.filter_languages_by_rules(words);

        if filtered_languages.len() == 1 {
            let filtered_language = filtered_languages.into_iter().next().unwrap();
            values.push((filtered_language, 1.0));
            return values;
        }

        let mut all_probabilities = Vec::<HashMap<Language, f64>>::new();
        let mut unigram_counts = HashMap::<Language, u32>::new();

        for i in 1..6 {
            if cleaned_up_text.chars().count() < i {
                continue;
            }
            let test_data_model = TestDataLanguageModel::from(&cleaned_up_text, i);
            let probabilities =
                self.compute_language_probabilities(&test_data_model, &filtered_languages);
            let languages = probabilities.keys().collect_vec();

            if !languages.is_empty() {
                filtered_languages = filtered_languages
                    .into_iter()
                    .filter(|it| languages.contains(&it))
                    .collect();
            }

            if i == 1 {
                self.count_unigrams(&mut unigram_counts, &test_data_model, &filtered_languages);
            }

            all_probabilities.push(probabilities);
        }

        let summed_up_probabilities =
            self.sum_up_probabilities(all_probabilities, unigram_counts, filtered_languages);

        if summed_up_probabilities.is_empty() {
            return values;
        }

        let highest_probability = summed_up_probabilities
            .iter()
            .map(|(_, &probability)| probability)
            .sorted_by(|&first, &second| second.partial_cmp(&first).unwrap())
            .next()
            .unwrap();

        let confidence_values = summed_up_probabilities
            .iter()
            .map(|(language, &probability)| (language.clone(), highest_probability / probability))
            .sorted_by(|(_, first_probability), (_, second_probability)| {
                second_probability.partial_cmp(first_probability).unwrap()
            })
            .collect_vec();

        confidence_values
    }

    fn clean_up_input_text(&self, text: String) -> String {
        let trimmed = text.trim().to_lowercase();
        let without_punctuation = PUNCTUATION.replace_all(&trimmed, "");
        let without_numbers = NUMBERS.replace_all(&without_punctuation, "");
        let normalized_whitespace = MULTIPLE_WHITESPACE.replace_all(&without_numbers, " ");
        normalized_whitespace.to_string()
    }

    fn split_text_into_words<'a>(&self, text: &'a str) -> Vec<&'a str> {
        if text.contains(' ') {
            text.split(' ').collect_vec()
        } else {
            vec![text]
        }
    }

    fn detect_language_with_rules(&self, words: &Vec<&str>) -> Option<Language> {
        let mut total_language_counts = HashMap::<Option<&Language>, u32>::new();
        let half_word_count = (words.len() as f64) * 0.5;

        for word in words {
            let mut word_language_counts = HashMap::<&Language, u32>::new();

            for character in word.chars() {
                let mut is_match = false;
                let mut buffer = [0; 4];
                let char_str = character.encode_utf8(&mut buffer);

                for (alphabet, language) in self.one_language_alphabets.iter() {
                    if alphabet.matches(char_str) {
                        self.increment_counter(&mut word_language_counts, language);
                        is_match = true;
                    }
                }

                if !is_match {
                    if Alphabet::Han.matches(char_str) {
                        self.increment_counter(&mut word_language_counts, &Chinese);
                    } else if JAPANESE_CHARACTER_SET.is_match(char_str) {
                        self.increment_counter(&mut word_language_counts, &Japanese);
                    } else if Alphabet::Latin.matches(char_str)
                        || Alphabet::Cyrillic.matches(char_str)
                        || Alphabet::Devanagari.matches(char_str)
                    {
                        self.languages_with_unique_characters
                            .iter()
                            .filter(|it| it.unique_characters().unwrap().contains(character))
                            .for_each(|it| self.increment_counter(&mut word_language_counts, it));
                    }
                }
            }

            if word_language_counts.is_empty() {
                self.increment_counter(&mut total_language_counts, None);
            } else if word_language_counts.len() == 1 {
                let counted_languages = word_language_counts.keys().collect_vec();
                let language = counted_languages.first().unwrap();
                if self.languages.contains(language) {
                    self.increment_counter(&mut total_language_counts, Some(language));
                } else {
                    self.increment_counter(&mut total_language_counts, None);
                }
            } else if word_language_counts.contains_key(&Chinese)
                && word_language_counts.contains_key(&Japanese)
            {
                self.increment_counter(&mut total_language_counts, Some(&Japanese));
            } else {
                let sorted_word_language_counts = word_language_counts
                    .into_iter()
                    .sorted_by(|(_, first_count), (_, second_count)| second_count.cmp(first_count))
                    .collect_vec();
                let (most_frequent_language, first_count) = sorted_word_language_counts[0];
                let (_, second_count) = sorted_word_language_counts[1];

                if first_count > second_count && self.languages.contains(most_frequent_language) {
                    self.increment_counter(
                        &mut total_language_counts,
                        Some(most_frequent_language),
                    );
                } else {
                    self.increment_counter(&mut total_language_counts, None);
                }
            }
        }

        let unknown_language_count = *total_language_counts.get(&None).or(Some(&0)).unwrap() as f64;

        if unknown_language_count < half_word_count {
            total_language_counts.remove(&None);
        }

        if total_language_counts.is_empty() {
            return None;
        }

        if total_language_counts.len() == 1 {
            return total_language_counts.iter().next().unwrap().0.cloned();
        }

        let sorted_total_language_counts = total_language_counts
            .into_iter()
            .sorted_by(|(_, first_count), (_, second_count)| second_count.cmp(first_count))
            .collect_vec();
        let (most_frequent_language, first_count) = sorted_total_language_counts[0];
        let (_, second_count) = sorted_total_language_counts[1];

        if first_count == second_count {
            return None;
        }

        most_frequent_language.cloned()
    }

    fn filter_languages_by_rules(&self, words: Vec<&str>) -> HashSet<Language> {
        let alphabets = vec![
            Alphabet::Arabic,
            Alphabet::Cyrillic,
            Alphabet::Devanagari,
            Alphabet::Han,
            Alphabet::Latin,
        ];
        let mut detected_alphabets = HashMap::<&Alphabet, u32>::new();
        let half_word_count = (words.len() as f64) * 0.5;

        for word in words.iter() {
            for alphabet in alphabets.iter() {
                if alphabet.matches(word) {
                    self.increment_counter(&mut detected_alphabets, alphabet);
                    break;
                }
            }
        }

        if detected_alphabets.is_empty() {
            return self.languages.clone();
        }

        let most_frequent_alphabet = detected_alphabets
            .into_iter()
            .sorted_by(|(_, first_count), (_, second_count)| second_count.cmp(first_count))
            .next()
            .unwrap()
            .0;

        let filtered_languages = self
            .languages
            .iter()
            .cloned()
            .filter(|it| it.alphabets().contains(most_frequent_alphabet))
            .collect::<HashSet<_>>();

        let mut language_counts = HashMap::<&Language, u32>::new();

        for word in words.iter() {
            for (characters, languages) in CHARS_TO_LANGUAGES_MAPPING.iter() {
                let mut word_contains_char = false;
                for character in characters.chars() {
                    if word.contains(character) {
                        for language in languages.iter() {
                            self.increment_counter(&mut language_counts, language);
                        }
                        word_contains_char = true;
                        break;
                    }
                }
                if word_contains_char {
                    break;
                }
            }
        }

        let languages_subset = language_counts
            .into_iter()
            .filter(|(_, count)| (*count as f64) >= half_word_count)
            .map(|(language, _)| language)
            .collect::<HashSet<_>>();

        if !languages_subset.is_empty() {
            filtered_languages
                .into_iter()
                .filter(|it| languages_subset.contains(&it))
                .collect::<HashSet<_>>()
        } else {
            filtered_languages
        }
    }

    fn compute_language_probabilities(
        &mut self,
        model: &TestDataLanguageModel,
        filtered_languages: &HashSet<Language>,
    ) -> HashMap<Language, f64> {
        let mut probabilities = hashmap!();
        for language in filtered_languages.iter() {
            let sum = self.compute_sum_of_ngram_probabilities(
                language,
                &model.ngrams,
                filtered_languages,
            );
            if sum < 0.0 {
                probabilities.insert(language.clone(), sum);
            }
        }
        probabilities
    }

    fn compute_sum_of_ngram_probabilities(
        &mut self,
        language: &Language,
        ngrams: &HashSet<Ngram>,
        filtered_languages: &HashSet<Language>,
    ) -> f64 {
        let mut probabilities = vec![];
        for ngram in ngrams.iter() {
            for elem in ngram.range_of_lower_order_ngrams() {
                let probability =
                    self.look_up_ngram_probability(language, &elem, filtered_languages);

                if probability > 0.0 {
                    probabilities.push(probability);
                    break;
                }
            }
        }
        probabilities.into_iter().map(|it| it.ln()).sum()
    }

    fn look_up_ngram_probability(
        &mut self,
        language: &Language,
        ngram: &Ngram,
        filtered_languages: &HashSet<Language>,
    ) -> f64 {
        let language_models = match ngram.value.chars().count() {
            5 => self.load_language_models(5, filtered_languages),
            4 => self.load_language_models(4, filtered_languages),
            3 => self.load_language_models(3, filtered_languages),
            2 => self.load_language_models(2, filtered_languages),
            1 => self.load_language_models(1, filtered_languages),
            0 => panic!("zerogram detected"),
            _ => panic!(
                "unsupported ngram length detected: {}",
                ngram.value.chars().count()
            ),
        };

        language_models
            .get(language)
            .unwrap()
            .get_relative_frequency(ngram)
    }

    fn count_unigrams(
        &mut self,
        unigram_counts: &mut HashMap<Language, u32>,
        unigram_model: &TestDataLanguageModel,
        filtered_languages: &HashSet<Language>,
    ) {
        for language in filtered_languages.iter() {
            for unigram in unigram_model.ngrams.iter() {
                if self.look_up_ngram_probability(language, unigram, filtered_languages) > 0.0 {
                    self.increment_counter(unigram_counts, language.clone());
                }
            }
        }
    }

    fn sum_up_probabilities(
        &self,
        probabilities: Vec<HashMap<Language, f64>>,
        unigram_counts: HashMap<Language, u32>,
        filtered_languages: HashSet<Language>,
    ) -> HashMap<Language, f64> {
        let mut summed_up_probabilities = hashmap!();
        for language in filtered_languages.iter() {
            let mut sum = probabilities
                .iter()
                .map(|it| match it.get(language) {
                    Some(probability) => *probability,
                    None => 0.0,
                })
                .sum();

            if unigram_counts.contains_key(language) {
                sum /= *unigram_counts.get(language).unwrap() as f64;
            }

            if sum != 0.0 {
                summed_up_probabilities.insert(language.clone(), sum);
            }
        }

        summed_up_probabilities
    }

    fn load_language_models(
        &mut self,
        ngram_length: u32,
        filtered_languages: &HashSet<Language>,
    ) -> &HashMap<Language, TrainingDataLanguageModel> {
        let map = match ngram_length {
            5 => &mut self.fivegram_language_models,
            4 => &mut self.quadrigram_language_models,
            3 => &mut self.trigram_language_models,
            2 => &mut self.bigram_language_models,
            1 => &mut self.unigram_language_models,
            _ => panic!("unsupported ngram length detected: {}", ngram_length),
        };
        for language in filtered_languages {
            if map.contains_key(language) {
                continue;
            }
            let json = load_json(&LANGUAGE_MODELS_DIRECTORY, language, ngram_length).unwrap();
            let model = TrainingDataLanguageModel::from_json(&json);
            map.insert(language.clone(), model);
        }
        map
    }

    fn increment_counter<T: Eq + Hash>(&self, counts: &mut HashMap<T, u32>, key: T) {
        let counter = counts.entry(key).or_insert(0);
        *counter += 1;
    }
}

fn load_json(directory: &Dir, language: &Language, ngram_length: u32) -> std::io::Result<String> {
    let ngram_name = Ngram::get_ngram_name_by_length(ngram_length);
    let file_path = format!("{}/{}s.json.zip", language.iso_code_639_1(), ngram_name);
    let zip_file = directory.get_file(file_path).unwrap();
    let zip_file_reader = Cursor::new(zip_file.contents());
    let mut archive = ZipArchive::new(zip_file_reader).unwrap();
    let mut json_file = archive.by_index(0).unwrap();
    let mut json = String::new();
    json_file.read_to_string(&mut json)?;
    Ok(json)
}

#[cfg(test)]
mod tests {
    use super::*;
    use include_dir::include_dir;
    use mockall::{predicate::*, *};
    use rstest::*;

    const LANGUAGE_MODELS_TEST_DIRECTORY: Dir = include_dir!("assets/test/language-models");

    // ##############################
    // MOCKS
    // ##############################

    mock! {
        pub(crate) TrainingDataLanguageModel {
            fn get_relative_frequency(&self, ngram: &Ngram) -> f64;
        }
    }

    fn create_language_model_mock(
        data: HashMap<&'static str, f64>,
    ) -> MockTrainingDataLanguageModel {
        let mut mock = MockTrainingDataLanguageModel::new();
        for (ngram, probability) in data {
            mock.expect_get_relative_frequency()
                .withf(move |n| n == &Ngram::new(ngram))
                .return_const(probability);
        }
        mock
    }

    // ##############################
    // LANGUAGE MODELS FOR ENGLISH
    // ##############################

    #[fixture]
    fn unigram_language_model_for_english() -> MockTrainingDataLanguageModel {
        create_language_model_mock(hashmap!(
            "a" => 0.01,
            "l" => 0.02,
            "t" => 0.03,
            "e" => 0.04,
            "r" => 0.05,
            // unknown unigrams
            "w" => 0.0
        ))
    }

    #[fixture]
    fn bigram_language_model_for_english() -> MockTrainingDataLanguageModel {
        create_language_model_mock(hashmap!(
            "al" => 0.11,
            "lt" => 0.12,
            "te" => 0.13,
            "er" => 0.14,
            // unknown bigrams
            "aq" => 0.0,
            "wx" => 0.0
        ))
    }

    #[fixture]
    fn trigram_language_model_for_english() -> MockTrainingDataLanguageModel {
        create_language_model_mock(hashmap!(
            "alt" => 0.19,
            "lte" => 0.2,
            "ter" => 0.21,
            // unknown trigrams
            "aqu" => 0.0,
            "tez" => 0.0,
            "wxy" => 0.0
        ))
    }

    #[fixture]
    fn quadrigram_language_model_for_english() -> MockTrainingDataLanguageModel {
        create_language_model_mock(hashmap!(
            "alte" => 0.25,
            "lter" => 0.26,
            // unknown quadrigrams
            "aqua" => 0.0,
            "wxyz" => 0.0
        ))
    }

    #[fixture]
    fn fivegram_language_model_for_english() -> MockTrainingDataLanguageModel {
        create_language_model_mock(hashmap!(
            "alter" => 0.29,
            // unknown fivegrams
            "aquas" => 0.0
        ))
    }

    // ##############################
    // LANGUAGE MODELS FOR GERMAN
    // ##############################

    #[fixture]
    fn unigram_language_model_for_german() -> MockTrainingDataLanguageModel {
        create_language_model_mock(hashmap!(
            "a" => 0.06,
            "l" => 0.07,
            "t" => 0.08,
            "e" => 0.09,
            "r" => 0.1,
            // unknown unigrams
            "w" => 0.0
        ))
    }

    #[fixture]
    fn bigram_language_model_for_german() -> MockTrainingDataLanguageModel {
        create_language_model_mock(hashmap!(
            "al" => 0.15,
            "lt" => 0.16,
            "te" => 0.17,
            "er" => 0.18,
            // unknown bigrams
            "wx" => 0.0
        ))
    }

    #[fixture]
    fn trigram_language_model_for_german() -> MockTrainingDataLanguageModel {
        create_language_model_mock(hashmap!(
            "alt" => 0.22,
            "lte" => 0.23,
            "ter" => 0.24,
            // unknown trigrams
            "wxy" => 0.0
        ))
    }

    #[fixture]
    fn quadrigram_language_model_for_german() -> MockTrainingDataLanguageModel {
        create_language_model_mock(hashmap!(
            "alte" => 0.27,
            "lter" => 0.28,
            // unknown quadrigrams
            "wxyz" => 0.0
        ))
    }

    #[fixture]
    fn fivegram_language_model_for_german() -> MockTrainingDataLanguageModel {
        create_language_model_mock(hashmap!("alter" => 0.3))
    }

    #[fixture]
    fn detector_for_english_and_german() -> LanguageDetector {
        LanguageDetector::from(hashset!(English, German), 0.0)
    }

    #[fixture]
    fn detector_for_all_languages() -> LanguageDetector {
        LanguageDetector::from(Language::all(), 0.0)
    }

    #[test]
    fn test_load_json() {
        let result = load_json(&LANGUAGE_MODELS_TEST_DIRECTORY, &Language::English, 1);
        assert!(result.is_ok());
        assert_eq!(
            result.unwrap(),
            r#"{"language":"ENGLISH","ngrams":{"2/93616591":"ï¬ Ä Ä Ä© È¼ É Å£ Å© Ê Æ¡ áº£ á» Ã¹"}}"#
        );
    }

    #[rstest]
    fn assert_text_is_cleaned_up_properly(detector_for_all_languages: LanguageDetector) {
        let text = "Weltweit    gibt es ungefÃ¤hr 6.000 Sprachen,
        wobei laut SchÃ¤tzungen zufolge ungefÃ¤hr 90  Prozent davon
        am Ende dieses Jahrhunderts verdrÃ¤ngt sein werden.";

        let expected_cleaned_text =
            "weltweit gibt es ungefÃ¤hr sprachen wobei laut schÃ¤tzungen zufolge ungefÃ¤hr \
            prozent davon am ende dieses jahrhunderts verdrÃ¤ngt sein werden";

        assert_eq!(
            detector_for_all_languages.clean_up_input_text(text.to_string()),
            expected_cleaned_text
        );
    }

    #[rstest]
    fn assert_text_is_split_into_words_correctly(detector_for_all_languages: LanguageDetector) {
        assert_eq!(
            detector_for_all_languages.split_text_into_words("this is a sentence"),
            vec!["this", "is", "a", "sentence"]
        );
        assert_eq!(
            detector_for_all_languages.split_text_into_words("sentence"),
            vec!["sentence"]
        );
    }

    #[rstest(
        word,
        expected_language,
        // words with unique characters
        case("mÉhÉrrÉm", Some(Azerbaijani)),
        case("substituÃ¯ts", Some(Catalan)),
        case("rozdÄlit", Some(Czech)),
        case("tvoÅen", Some(Czech)),
        case("subjektÅ¯", Some(Czech)),
        case("nesufiÄecon", Some(Esperanto)),
        case("intermiksiÄis", Some(Esperanto)),
        case("monaÄ¥inoj", Some(Esperanto)),
        case("kreitaÄµoj", Some(Esperanto)),
        case("Åpinante", Some(Esperanto)),
        case("apenaÅ­", Some(Esperanto)),
        case("groÃ", Some(German)),
        case("ÏÏÎ­Î´Î¹Î±", Some(Greek)),
        case("fekvÅ", Some(Hungarian)),
        case("meggyÅ±rÅ±zni", Some(Hungarian)),
        case("ã´ã§ãã¤ã¤ã¢ã³ã", Some(Japanese)),
        case("ÓÐ»ÐµÐ¼", Some(Kazakh)),
        case("ÑÐ°ÑÑÐ°ÑÑÐ»ÑÒÑ", Some(Kazakh)),
        case("Ð°ÒÑÐ½", Some(Kazakh)),
        case("Ð¾Ð½ÑÒ£", Some(Kazakh)),
        case("ÑÒ±ÑÐ°Ð¹Ð»Ñ", Some(Kazakh)),
        case("teoloÄ£iska", Some(Latvian)),
        case("blaÄ·ene", Some(Latvian)),
        case("ceÄ¼ojumiem", Some(Latvian)),
        case("numuriÅu", Some(Latvian)),
        case("mergelÄs", Some(Lithuanian)),
        case("Ä¯rengus", Some(Lithuanian)),
        case("slegiamÅ³", Some(Lithuanian)),
        case("Ð¿ÑÐ¸Ð¿Ð°ÑÐ°", Some(Macedonian)),
        case("ÑÐ¸Ð´Ð¾Ð²Ð¸", Some(Macedonian)),
        case("ÑÐµÑÐºÐ°", Some(Macedonian)),
        case("ÑÐ°Ð¼Ð¸Ð¸ÑÐµ", Some(Macedonian)),
        case("à¤®à¤¿à¤³à¤¤à¥", Some(Marathi)),
        case("Ò¯Ð½Ð´ÑÑÐ½", Some(Mongolian)),
        case("Ð´Ó©ÑÓ©Ð¶", Some(Mongolian)),
        case("zmieniÅy", Some(Polish)),
        case("paÅstwowych", Some(Polish)),
        case("mniejszoÅci", Some(Polish)),
        case("groÅºne", Some(Polish)),
        case("ialomiÅ£a", Some(Romanian)),
        case("Ð½Ð°ÑÐ»ÐµÑÐ¸Ð²Ð°ÑÐ°", Some(Serbian)),
        case("Ð½ÐµÐ¸ÑÐºÐ²Ð°ÑÐµÐ½Ð¾ÑÑÑ", Some(Serbian)),
        case("podÄºa", Some(Slovak)),
        case("pohÄ¾ade", Some(Slovak)),
        case("mÅtvych", Some(Slovak)),
        case("ÒÑÑÐ½ÑÐ¾Ð²Ð¾Ð¼Ñ", Some(Ukrainian)),
        case("Ð¿ÑÐ¾Ð¿Ð¾Ð½ÑÑ", Some(Ukrainian)),
        case("Ð¿ÑÐ¸ÑÑÑÐ¾Ñ", Some(Ukrainian)),
        case("cáº±m", Some(Vietnamese)),
        case("tháº§n", Some(Vietnamese)),
        case("cháº³ng", Some(Vietnamese)),
        case("quáº©y", Some(Vietnamese)),
        case("sáºµn", Some(Vietnamese)),
        case("nháº«n", Some(Vietnamese)),
        case("dáº¯t", Some(Vietnamese)),
        case("cháº¥t", Some(Vietnamese)),
        case("Äáº¡p", Some(Vietnamese)),
        case("máº·n", Some(Vietnamese)),
        case("háº­u", Some(Vietnamese)),
        case("hiá»n", Some(Vietnamese)),
        case("láº»n", Some(Vietnamese)),
        case("biá»u", Some(Vietnamese)),
        case("káº½m", Some(Vietnamese)),
        case("diá»m", Some(Vietnamese)),
        case("pháº¿", Some(Vietnamese)),
        case("viá»c", Some(Vietnamese)),
        case("chá»nh", Some(Vietnamese)),
        case("trÄ©", Some(Vietnamese)),
        case("ravá»", Some(Vietnamese)),
        case("thÆ¡", Some(Vietnamese)),
        case("nguá»n", Some(Vietnamese)),
        case("thá»", Some(Vietnamese)),
        case("sá»i", Some(Vietnamese)),
        case("tá»ng", Some(Vietnamese)),
        case("nhá»", Some(Vietnamese)),
        case("má»i", Some(Vietnamese)),
        case("bá»¡i", Some(Vietnamese)),
        case("tá»t", Some(Vietnamese)),
        case("giá»i", Some(Vietnamese)),
        case("má»t", Some(Vietnamese)),
        case("há»£p", Some(Vietnamese)),
        case("hÆ°ng", Some(Vietnamese)),
        case("tá»«ng", Some(Vietnamese)),
        case("cá»§a", Some(Vietnamese)),
        case("sá»­", Some(Vietnamese)),
        case("cÅ©ng", Some(Vietnamese)),
        case("nhá»¯ng", Some(Vietnamese)),
        case("chá»©c", Some(Vietnamese)),
        case("dá»¥ng", Some(Vietnamese)),
        case("thá»±c", Some(Vietnamese)),
        case("ká»³", Some(Vietnamese)),
        case("ká»·", Some(Vietnamese)),
        case("má»¹", Some(Vietnamese)),
        case("má»µ", Some(Vietnamese)),
        case("kÅnin", Some(Yoruba)),
        case("á¹£aaju", Some(Yoruba)),
        case("ÙØ§ÙÙÙØ¶ÙØ¹", None),
        case("ÑÐ¾Ð¿ÑÐ¾ÑÐ¸Ð²Ð»ÐµÐ½Ð¸Ðµ", None),
        case("house", None),

        // words with unique alphabet
        case("Õ¸ÖÕ¶Õ¥Õ¶Õ¡", Some(Armenian)),
        case("à¦à¦¾à¦¨à¦¾à¦¤à§", Some(Bengali)),
        case("ááá áá£ááá", Some(Georgian)),
        case("ÏÏÎ±Î¼Î¬ÏÎ·ÏÎµ", Some(Greek)),
        case("àªàªªàªàª°àª£à«àª¨à«", Some(Gujarati)),
        case("××ª××¨××××ª", Some(Hebrew)),
        case("ã³ã", Some(Japanese)),
        case("ëê²°êµ¬ëê°", Some(Korean)),
        case("à¨®à©à¨à¨°à¨¸à¨¾à¨à¨à¨²à¨¾à¨", Some(Punjabi)),
        case("à®¤à¯à®©à¯à®ªà®à¯à®à®³à¯", Some(Tamil)),
        case("à°à±à°·à±à°£à°¦à±à°µà°°à°¾à°¯à°²à±", Some(Telugu)),
        case("à¹à¸à¸à¸²à¸à¸«à¸¥à¸§à¸à¸«à¸¡à¸²à¸¢à¹à¸¥à¸", Some(Thai)),
    )]
    fn assert_language_detection_with_rules_works_correctly(
        detector_for_all_languages: LanguageDetector,
        word: &str,
        expected_language: Option<Language>,
    ) {
        let detected_language = detector_for_all_languages.detect_language_with_rules(&vec![word]);
        assert_eq!(
            detected_language, expected_language,
            "expected {:?} for word '{}', got {:?}",
            expected_language, word, detected_language
        );
    }

    #[rstest(word, expected_languages,
        case("ÙØ§ÙÙÙØ¶ÙØ¹", hashset!(Arabic, Persian, Urdu)),
        case(
            "ÑÐ¾Ð¿ÑÐ¾ÑÐ¸Ð²Ð»ÐµÐ½Ð¸Ðµ",
            hashset!(
                Belarusian, Bulgarian, Kazakh, Macedonian, Mongolian, Russian, Serbian, Ukrainian
            )
        ),
        case("ÑÐ°ÑÐºÑÑÐ²Ð°Ðµ", hashset!(Belarusian, Kazakh, Mongolian, Russian)),
        case("ÑÑÐ¾Ñ", hashset!(Belarusian, Kazakh, Mongolian, Russian)),
        case("Ð¾Ð³Ð½ÑÐ¼", hashset!(Belarusian, Kazakh, Mongolian, Russian)),
        case("Ð¿Ð»Ð°Ð²Ð°ÑÐ°", hashset!(Bulgarian, Kazakh, Mongolian, Russian)),
        case("Ð´Ð¾Ð²ÑÑÑÐ°Ñ", hashset!(Bulgarian, Kazakh, Mongolian, Russian)),
        case("Ð¿Ð°Ð²ÑÐ½ÐµÐ½", hashset!(Belarusian, Kazakh, Ukrainian)),
        case("Ð·Ð°ÑÐ¾Ð¿Ð»ÑÐ²Ð°ÑÐµ", hashset!(Macedonian, Serbian)),
        case("ÑÐµÐºÑÐ°ÑÑÐµÐ½Ð·Ð¸ÑÐ°", hashset!(Macedonian, Serbian)),
        case("Ð½Ð°Ð±ÑÑÐ´ÑÐ²Ð°Ñ", hashset!(Macedonian, Serbian)),
        case("aizklÄtÄ", hashset!(Latvian, Yoruba)),
        case("sistÄmas", hashset!(Latvian, Yoruba)),
        case("palÄ«dzi", hashset!(Latvian, Yoruba)),
        case("nháº¹n", hashset!(Vietnamese, Yoruba)),
        case("chá»n", hashset!(Vietnamese, Yoruba)),
        case("prihvaÄanju", hashset!(Bosnian, Croatian, Polish)),
        case("naÄete", hashset!(Bosnian, Croatian, Vietnamese)),
        case("visÃ£o", hashset!(Portuguese, Vietnamese)),
        case("wystÄpiÄ", hashset!(Lithuanian, Polish)),
        case("budowÄ", hashset!(Lithuanian, Polish)),
        case("nebÅ«sime", hashset!(Latvian, Lithuanian, Yoruba)),
        case("afiÅate", hashset!(Azerbaijani, Romanian, Turkish)),
        case("kradzieÅ¼ami", hashset!(Polish, Romanian)),
        case("Ã®nviat", hashset!(French, Romanian)),
        case("venerdÃ¬", hashset!(Italian, Vietnamese, Yoruba)),
        case("aÃ±os", hashset!(Basque, Spanish)),
        case("rozohÅuje", hashset!(Czech, Slovak)),
        case("rtuÅ¥", hashset!(Czech, Slovak)),
        case("pregÄtire", hashset!(Romanian, Vietnamese)),
        case("jeÄte", hashset!(Czech, Romanian, Slovak)),
        case("minjaverÃ°ir", hashset!(Icelandic, Latvian, Turkish)),
        case("Ã¾agnarskyldu", hashset!(Icelandic, Latvian, Turkish)),
        case("nebÃ»tu", hashset!(French, Hungarian, Latvian)),
        case("hashemidÃ«ve", hashset!(Afrikaans, Albanian, Dutch, French)),
        case("forÃªt", hashset!(Afrikaans, French, Portuguese, Vietnamese)),
        case("succÃ¨dent", hashset!(French, Italian, Vietnamese, Yoruba)),
        case("oÃ¹", hashset!(French, Italian, Vietnamese, Yoruba)),
        case("tÃµeliseks", hashset!(Estonian, Hungarian, Portuguese, Vietnamese)),
        case("viÃ²iem", hashset!(Catalan, Italian, Latvian, Vietnamese, Yoruba)),
        case("contrÃ´le", hashset!(French, Portuguese, Slovak, Vietnamese)),
        case("direktÃ¸r", hashset!(Bokmal, Danish, Nynorsk)),
        case("vÃ½voj", hashset!(Czech, Icelandic, Slovak, Turkish, Vietnamese)),
        case("pÃ¤ralt", hashset!(Estonian, Finnish, German, Slovak, Swedish)),
        case("labÃ¢k", hashset!(Latvian, Portuguese, Romanian, Turkish, Vietnamese)),
        case("prÃ ctiques", hashset!(Catalan, French, Italian, Portuguese, Vietnamese)),
        case("Ã¼berrascht", hashset!(Azerbaijani, Catalan, Estonian, German, Hungarian, Turkish)),
        case("indebÃ¦rer", hashset!(Bokmal, Danish, Icelandic, Nynorsk)),
        case("mÃ¥ned", hashset!(Bokmal, Danish, Nynorsk, Swedish)),
        case("zaruÄen", hashset!(Bosnian, Czech, Croatian, Latvian, Lithuanian, Slovak, Slovene)),
        case("zkouÅ¡kou", hashset!(Bosnian, Czech, Croatian, Latvian, Lithuanian, Slovak, Slovene)),
        case("navrÅ¾en", hashset!(Bosnian, Czech, Croatian, Latvian, Lithuanian, Slovak, Slovene)),
        case(
            "faÃ§onnage",
            hashset!(Albanian, Azerbaijani, Basque, Catalan, French, Latvian, Portuguese, Turkish)
        ),
        case(
            "hÃ¶her",
            hashset!(Azerbaijani, Estonian, Finnish, German, Hungarian, Icelandic, Swedish, Turkish)
        ),
        case(
            "catedrÃ¡ticos",
            hashset!(
                Catalan, Czech, Icelandic, Irish, Hungarian, Portuguese, Slovak, Vietnamese, Yoruba
            )
        ),
        case(
            "polÃ­tica",
            hashset!(
                Catalan, Czech, Icelandic, Irish, Hungarian, Portuguese, Slovak, Vietnamese, Yoruba
            )
        ),
        case(
            "mÃºsica",
            hashset!(
                Catalan, Czech, Icelandic, Irish, Hungarian, Portuguese, Slovak, Vietnamese, Yoruba
            )
        ),
        case(
            "contradicciÃ³",
            hashset!(
                Catalan, Hungarian, Icelandic, Irish, Polish, Portuguese, Slovak, Vietnamese, Yoruba
            )
        ),
        case(
            "nomÃ©s",
            hashset!(
                Catalan, Czech, French, Hungarian, Icelandic, Irish, Italian, Portuguese, Slovak,
                Vietnamese, Yoruba
            )
        ),
        case(
            "house",
            hashset!(
                Afrikaans, Albanian, Azerbaijani, Basque, Bokmal, Bosnian, Catalan, Croatian, Czech,
                Danish, Dutch, English, Esperanto, Estonian, Finnish, French, Ganda, German, Hungarian,
                Icelandic, Indonesian, Irish, Italian, Latin, Latvian, Lithuanian, Malay, Nynorsk,
                Polish, Portuguese, Romanian, Shona, Slovak, Slovene, Somali, Sotho, Spanish, Swahili,
                Swedish, Tagalog, Tsonga, Tswana, Turkish, Vietnamese, Welsh, Xhosa, Yoruba, Zulu
            )
        ),
    )]
    fn assert_language_filtering_with_rules_works_correctly(
        detector_for_all_languages: LanguageDetector,
        word: &str,
        expected_languages: HashSet<Language>,
    ) {
        let filtered_languages = detector_for_all_languages.filter_languages_by_rules(vec![word]);
        assert_eq!(
            filtered_languages, expected_languages,
            "expected {:?} for word '{}', got {:?}",
            expected_languages, word, filtered_languages
        );
    }

    #[rstest(invalid_str, case(""), case(" \n  \t;"), case("3<856%)Â§"))]
    fn assert_strings_without_letters_return_no_language(
        mut detector_for_all_languages: LanguageDetector,
        invalid_str: &str,
    ) {
        assert_eq!(
            detector_for_all_languages.detect_language_of(invalid_str),
            None
        );
    }
}
